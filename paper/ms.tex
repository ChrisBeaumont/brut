\documentclass[preprint]{aastex}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}

\usepackage{graphicx}
\makeatletter
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex,
              colorlinks=true,
              linkcolor=blue]{hyperref}
\else
  \usepackage[unicode=true,
              colorlinks=true,
              linkcolor=blue]{hyperref}
\fi

\newcommand{\todo}[1]{{\color[rgb]{0, .5, .5} TODO: #1}}
\newcommand{\um}[0]{$\mu$m}

\begin{document}


\title{An Algorithmically-Derived Catalog of Bubbles in the Galaxy}
\shortauthors{Beaumont et al}
\author{Christopher N. Beaumont$^{1,2}$, Alyssa A. Goodman$^2$, Sarah Kendrew$^3$, Jonathan P. Williams$^1$, Robert Simpson$^3$}
\affil{$^1$Institute for Astronomy, University of Hawai'i, 2680 Woodlawn Drive, Honolulu HI 96822;  beaumont@ifa.hawaii.edu}
\affil{$^2$Harvard-Smithsonian Center for Astrophysics, 60 Garden St., Cambridge MA 02138}
\affil{$^3$Department of Astrophysics, University of Oxford, Denys Wilkinson Building, Keble Road, Oxford OX1 3RH, UK}


\section{Introduction}
\label{sec:intro}

\subsection{Interstellar Bubbles}
\label{sec:bubbles}

\subsection{Previous Surveys}
\label{sec:previous_surveys}

\begin{figure}
\includegraphics{mwp}
\caption{Example images presented to Milky Way Project citizen scientists to identify bubbles. The images show 4.5$\mu$m emission in blue, 
8$\mu$m emission in green, and 24$\mu$m emission in red.}
\end{figure}

\begin{figure*}
\includegraphics[trim= .7in 0 0 0, clip]{gallery}
\caption{Different astrophysical objects in the MWP catalog. a) ``Canonical'' wind-blown bubbles and HII regions. b) shells
without 8 $\mu$m PAH emission (likely supernovae and planetary nebulae). c) 24 $\mu$m-dark filaments and globules. d) generic
ISM structures of unclear astrophysical origin.}
\label{fig:gallery}
\end{figure*}

\subsection{Manual and Automatic Classification in Astronomy}
\label{sec:benefits}

In terms of accuracy, humans still outperform computers in most image-based pattern recognition tasks. Because of this, morphologically complex
structures in the ISM (including supernova remnants, outflows, bubbles, HII regions, molecular and infrared dark clouds, and planetary nebulae) are still
traditionally cataloged manually. Human classification has several disadvantages, however.

First, human classification is time consuming, and people-hours are a limited resource. Even by enlisting large communities of citizen scientists, data from next generation surveys will be too large to search exhaustively. For example, the $>$ 35,000 citizen scientists of the Milky Way Project classified roughly 45 GB of image data from the GLIMPSE I and II surveys. Current and next-generation astronomical datasets are many thousands of times larger than this, suggesting tens of millions of citizen scientists would be needed for similar exhaustive, manual searches.

Second, many scientifically important tasks are not suitable for enlisting the public. Part of the success of the Milky Way Project is due to the fact that the GLIMPSE images are beautiful, contain many bubbles, and are compelling to browse through. Searches for very rare or less aesthetic objects, on the other hand, are more tedious and less likely to entice large citizen science communities. 

Finally, manual classification is not easily repeatable, and hard to calibrate statistically. For example, while the MWP catalog does include a "hit-rate" denoting the fraction of users who agree upon the existence of each bubble, there is no quantitative estimate of the reliability of each bubble. Furthermore, the current set of classifications cannot be directly applied to new data.

Automatic classifications driven by machine learning techniques nicely complement human classification. Such an approach easily scales to large data volumes, and is immune to some of the factors that affect humans, like boredom and fatigue. Furthermore, because algorithmic classifications are systematic and repeatable, they are easier to interpret and statistically characterize. 

The principal downside to automated classification for ``hard'' tasks like bubble detection is that the algorithms typically require a ``training set'' of example objects to classify. Fortunately, the Milky Way Project catalogs just such a training set. We build an automatic bubble detector from these data in the present paper. There are several reasons to do this:

\begin{enumerate}
\item We can search for bubbles not detected by MWP citizen scientists. The main reason users may have missed bubbles is because they were presented with images at a fixed contrast. Thus, faint bubbles near bright sources may have gone undetected.

\item The automatic classifier is capable of producing quantitative reliability estimates for each bubble in the MWP catalog, potentially flagging non-bubble interlopers and leading to a cleaner catalog.

\item We can treat this task as a case study for complex classification tasks in future datasets, where exhaustive manual classification will not be feasible.

\end{enumerate}


\section{Classification Method}
\label{sec:method}

Our goal is to use the set of citizen-science classifications to build an automatic detector that, when presented with a region of the sky in the \textit{Spitzer} GLIMPSE and MIPSGAL surveys, accurately determines whether or not the image contains a bubble. This is a typical case of a supervised learning problem. Before we explain our approach in detail, here is a brief overview of the task:

\begin{enumerate}
\item Build a representative training set of examples of bubble and non-bubble images. This will be drived from the MWP dataset
\item Convert each example to a numerical \textit{feature vector} that describes each object and, hopefully, distinguishes between bubbles and non-bubbles
\item Feed the training data to a learning algorithm to build a model
\item Use a subset of the examples not used during training to optimize the tunable parameters (so called \textit{hyper parameters}) of the learning algorithm
\end{enumerate}

\subsection{Training Data}
\label{sec:method_training_data}
The MWP catalog is a natural choice to build a training dataset. However, as shown in Figure \ref{fig:gallery}, the catalog as a whole contains many examples of objects that are not bubbles, and have the potential to confuse the training process. Thus, we manually curated a list of 468 objects in the catalog which were clear examples of bubbles. 

We also require an example set of non-bubbles. In principle, any random region of the sky that doesn't overlap an object in the MWP catalog is a candidate negative example -- while some of these fields actually contain bubbles not in the MWP catalog, they are rare enough that they don't strongly impact the training. However, we found that selecting negative examples at random was sub-optimal. The reason for this is that most random fields are nearly empty, and easily distinguished from fields with bubbles. Classifiers generated from such a dataset incorrectly labeled nearly any field containing extended emission as a bubble.

A better set of negative examples includes more examples of fields containing extended emission (Figure \ref{fig:bootstrap_neg}). We built such a collection in a bootstrapped fashion. Starting with random negative examples, we trained a classifier and used it to scan a large number (20,000) of negative fields. We then discarded half of the initial negative examples (those classified most confidently as not containing bubbles), and replaced them with a random sample of the mis-classified examples. We repeated this process several times, but found that one iteration was usually sufficient to build a good set of training data.

\begin{figure}
\includegraphics{bootstrap_neg}
\caption{Examples of typical randomly selected negative examples (left), and ``hard''` examples (right)}
\label{fig:bootstrap_neg}
\end{figure}

\subsection{Feature Extraction}
\label{sec:method_feature_extraction}
Supervised classification algorithms distinguish between classes of objects based on numerical \textit{feature vectors} that summarize each object. Feature vector design is often the most important stage of supervised classification and, unfortunately, also the hardest. The ideal feature vector captures the differences between each class of objects, so that examples from different classes occupy different sub-regions of feature space. 

In the case of image classification, the most obvious choice for a feature vector is simply the numerical value of the pixels themselves. This turns out to be a poor choice, because extended objects like bubbles are characterized by correlations between hundreds or thousands of pixels, and any individual pixel is virtually irrelevant to the classification task. Machine learning algorithms generally perform better when individual elements of the feature vector are more important to the classification, and less dependent on the value of other feature vector elements.

Our feature vectors have been designed based on insights in the automated face detection literature \citep{ViolaJones}.  The basic strategy is to encode a very large number ($\sim$40,000) of generic statistics about extended structures at various positions and scales. This strategy seems to be more effective then trying to tune a feature vector to the specific object being identified. While most elements in the feature vector will have no predictive power in the classification, the appropriate learning algorithm will simply ignore this information.

To construct our feature vector for a given region in the sky, we first extract a (40x40) pixel postage stamp of the field from IRAC 8 $\mu$m and MIPS 24 $\mu$m images. Following a scheme similar to \cite{mwp1}, these images are intensity clipped at the 1 and 97th percentile, normalized to maximum intensity of 1, and passed through a square root transfer function. This scaling tends to do a good job of emphasizing ISM structure, making bubbles more visible to the eye. Then, the following properties were extracted from each postage stamp, and concatenated to form the feature vector:

\begin{enumerate}
\item The wavelet coefficients from the Discrete Cosine and Daub4 wavelet transforms. These coefficients encode how much power is in various spatial frequencies.
\item The image dot product\footnote{i.e., the sum of the pixel-by-pixel product of two images} of each image with template images of rings of different size and thickness. Bubbles are morphologically similar to these templates, and presumably have larger dot products than a random intensity distribution.
\item The byte size of each compressed postage stamp image -- images substantially more or less smooth than a bubble compress to smaller and larger files, respectively.
\item The DAISY features of the image \citep{DAISY}, which characterize the local gradients in an image and have been useful in other feature recognition contexts.
\end{enumerate}

\subsection{Training}
While there are many algorithms designed to learn class labels form a training set of feature vectors, we focus here on the Random Forest algorithm \citep{random forest}. Random Forests are aggregates of a simpler learning algorithm called a decision tree. A decision tree makes classifications by passing each feature vector through the nodes of a tree diagram. Each node consists of a simple boolean condition on one element of the feature vectors (e.g., $F_3 > 5$). Examples traverse the left or right paths through the tree based on whether each test is True or False, until they reach a terminal node. The terminal node contains the final classification (in the present context, ``Bubble'' or ``Not Bubble''). Decision trees are constructed in a greedy, stage-wise fashion: each boolean test is chosen to separate the set of training data into two subclasses that are more cleanly partitioned between classes (several different metrics can be used to measure the ``purity'' of the subgroups).

On their own, decision trees are prone to over-fitting the training data by adding too many nodes. Random Forests overcome this problem by building several trees on different subsets of the data. The final classification is made by taking the majority vote of each subtree. This mitigates over-fitting in the individual trees, since over-fitting is sensitive to the specific subset of data used, and hence averages away. Random Forests have proven effective in many machine learning contexts.

In addition to their good performance and robustness against over fitting, Random Forests have several other advantageous properties: it is possible to interpret how important each element in the feature vector is to the classification, the algorithm naturally ignores irrelevant elements of the feature vector, and it is conceptually and computationally simple.

\subsection{Hyperparameter Optimization}
The Random Forest algorithm has a small number of tunable parameters. We determined good settings for these hyper-parameters using cross validation. We partitioned the training data into two sets, and used the first set to train the algorithm with a given set of hyper-parameters. The second set of cross-validation data was used to evaluate the model. Then, the hyper parameters were adjusted and the process repeated. We chose the hyper parameters that maximized the performance on the cross-validation data.

\subsection{Final Model}

Classifiers cannot be used to re-classify the data used to train them, due to the possibility of over-fitting. Thus, to obtain valid classifications for all fields, we constructed three classifiers. Each classifier is trained using data from two-thirds of the sky, and used to classify the remaining third. The sky is partitioned into thirds based on the modulus of the nearest whole longitude: for example, the first classifier is trained on examples where $0.5 < (\ell \mod{3})  < 2.5$, and used to classify fields where ($\ell \mod{3}) < 0.5 ~ || ~(\ell \mod{3}) > 2.5 $

\section{Results}
\label{sec:results}

The output of the RandomForest classifier is a score, which represents the fraction of trees in the forest which classify a feature vector as a bubble. This score provides more information than a simple binary classification, as it gives some sense of the confidence of the classification. Furthermore, one can adjust the threshold that defines when an object is determined to be a bubble. Increasing the threshold increases the reliability of the catalog, at the cost of completeness.

One way to summarize the performance of a classifier is to plot the false positive rate (fraction of negative examples incorrectly classified) versus the true positive rate (fraction of bubbles currently classified) as the threshold varies. This is (unfortunately) referred to as the Receiver Operating Characteristic, and is shown for the three classifiers in Figure \ref{fig:roc}. The false positive rate is measured by classifying $\sim 15000$ random negative fields, and the true positive rate is measured using the high-quality subset of the MWP catalog described above. Recall that the set of bubbles in this test set were selected because they are very well defined. Thus, Figure \ref{fig:roc} overestimates the true positive rate when presented with more ambiguous bubbles. We compare the performance of the classifier on more representative bubbles in Section \ref{sec:case_study} and \ref{sec:user_study}.

\begin{figure}
\includegraphics{roc}
\caption{ROC}
\label{fig:roc}
\end{figure}


\begin{figure}
\includegraphics{fp}
\caption{FP}
\label{fig:fp}
\end{figure}


\subsection{Building a Catalog}
Using our automatic bubble classifier to conduct a full search for Bubbles in \textit{Spitzer} data is straightforward. The main task involves scanning the classifier over all images, at all scales. At each of these postage stamps, the classifier yields a score, representing its confidence that the given field contains a bubble. A simple threshold on this score identifies regions likely to contain a bubble -- from the discussion of Figure \ref{fig:roc} above, adjusting this threshold sets the tradeoff between completeness and reliability. 

In practice, the classifier is insensitive to small adjustments to the size or location of the field. As a result, bubbles are usually associated with a cluster of postage stamps that rise above the detection threshold (the grey boxes in Figure \ref{fig:cluster}). We follow a simple, greedy procedure to merge these individual detections. Given a collection of postage stamp descriptions (location and scale), this procedure identifies the two nearest locations at similar scales, and discards the location assigned a lower score by the classifier This process repeats until no two fields are close enough to discard. Two fields are close enough to merge if they are separated by less than the radius of either field, and have sizes that differ by less than 50\%. The purple box in Figure \ref{fig:cluster}) shows the result of clustering.

\begin{figure}
\includegraphics{cluster}
\caption{An illustration of how Brut merges multiple bubble detections.}
\label{fig:cluster}
\end{figure}
 
 
This practice works well for isolated bubbles, but occasionally fails for bubble complexes with multiple, overlapping bubbles. A more sophisticated clustering analysis may well perform better in this situation.

\subsection{Classifying the l=35 field}
\label{sec:case_study}
To better understand the successes and failures of our classifier, we have analyzed the longitude range $34.5^\circ < \ell < 35.5^\circ$ in detail. In our first analysis, we scanned for all bubbles above a minimum size $r _{\rm min} = 80''$ \todo{Is this r or d?}, with scores above $s_{\rm min} = 0.55$. This search yielded 25 bubbles after merging. 

Among the 25 automatic detections, 13 are unambiguous bubbles with clear counterparts in the MWP catalog. A further four objects appear to be bubbles as judged by the authors, but are part of bubble complexes. While the complexes are identified at least once in both catalogs, each catalog decomposes the complex slightly differently. Thus, these 4 objects don't have an obvious one-to-one relationship with an object in the MWP catalog. Figure \ref{fig:cluster_confusion} shows an example of one region of confusion. The automated classifier discovers two new candidate bubbles with no counterpart in the MWP catalog. \todo{Lookup what the MWP images for these regions look like. Why weren't they detected? My guess is contrast}. The remaining 6 objects in the automated catalog are false positives -- three are regions containing a bright and/or extended 24\um\, point source confused for a bubble interior, and three are regions with 8 \um\, nebulosity that don't appear shell-like.

\begin{figure}
\includegraphics{cluster_confusion}
\caption{An example cluster of bubbles. The Brut and MWP catalogs tend to decompose these complex regions differently.}
\label{fig:cluster_confusion}
\end{figure}
 
In this same region, there are 32 objects in the MWP catalog, and 11 are unassociated with the automatic detections. Seven of these are judged by the authors to be false positives -- they are mostly regions of 8 \um\, nebulosity that don't appear to be shell-like or associated with young stars.  Five appear to be true bubbles, wrongly rejected by the automated classifier. The remaining six bubbles were not considered by the classifier -- three were too close to the image edge to be analyzed, and three were smaller than the minimum search scale (we return to these objects in the next subsection). 

These numbers allow us to estimate the reliability and completeness of both catalogs. Assuming that there are no other bubbles in this region with $r > s_{\rm min}$ missed by both classifiers (but still detectable from the image data), there are roughly 24 detectable bubbles in this region -- 13 bubbles present in both catalogs, 5 objects only in the MWP catalog, 2 objects only in the automated catalog, and $\sim$ 4 bubbles in bubble complexes. The automated catalog recovers 80\% (19/24) of the bubbles, and mis-classifies 24\% (6/25) of detections. The Milky Way Project performs similarly: it recovers 92\% of bubbles (22/24), and mis-classifies 23\% of its detections (7/31).

\subsubsection{Searching for small bubbles}
In the previous analysis, three bubbles in the MWP catalog were not included in the automated catalog because they were smaller than the smallest scales scanned over.  These objects would otherwise have been included -- that is, the classifier assigns each bubble a high score. We thus repeated the above analysis, with a lower value of $s_{\rm min}$. This smaller-scale scan recovers the three small bubbles missed by the first scan. In addition, it discovers 4 additional small bubbles not in the MWP catalog. \todo{Are these in the small object catalog?}. However, false positive rate is also higher -- at very small scales, the classifier more easily confuses bright 24 \um\, sources as bubble interiors, and find's more coincidental 8\um\, nebulosity that is vaguely ring-like. Overall, the false positive rate is 42\% when including a search for small bubbles, and detections not in the MWP catalog are 4 times more likely to be false positives than legitimate bubbles. From this, we conclude that our classifier is not currently suitable for searching for small bubbles.

\subsection{Expert Validation}

At some level, identifying bubbles in \textit{Spitzer} images is a subjective task. While the objects in Figure \ref{fig:gallery}a are unambiguously associated with star formation, other sources are not. In some cases -- particularly irregular bubbles, or those with faint or no 24\um\, emission -- it is unclear whether cavities seen in 8 \um\, data are actively sculpted by young stars, or merely the underlying structure of the undisturbed ISM. A proper analysis of a bubble detection strategy should account for this.

The subjectivity of bubble identification has received only limited attention. The MWP catalog includes a ``hit rate'' measurement that lists the fraction of citizen scientists who identify a specific bubble, relative to the total number of users who viewed an image containing that bubble. However, as discussed above, the Milky Way Project catalog contains many objects besides bubbles. Thus, while the hit rate communicates how visually salient a particular ISM feature appears to MWP users, it does not directly convey how much consensus astronomers have about the astrophysical nature of that feature.

To better measure the level of expert consensus in bubble identification, we conducted a small online survey. The astronomer participants of this survey were presented with a sequence of 92 Spitzer images at three zoom levels and two contrast settings. They were asked to assign each image to one of three categories: clear examples of bubbles or H$_{II}$ regions, ambiguous or irregular bubbles, or Non-bubbles. Appendix A discusses the survey setup in more detail.

\subsubsection{Validation of the Milky Way Project Catalog}
\label{sec:expert_mwp}
Of the 92 images in the expert survey, 45 are a random subset of objects in the MWP catalog. Figure \ref{fig:expert_mwp_votes} shows the voting breakdown for these objects. Each row corresponds to a single image, and the red, orange, and blue bars show the fraction of experts who classified the image in each category. The darkened color in each row indicates the most popular classification.  We call attention to several aspects of this distribution: 


\begin{figure}
\includegraphics{expert_mwp_votes}
\caption{The distribution of expert classifications for 41 bubbles in the Milky Way Project catalog. Each row corresponds to an image, and the colored bars in that row indicate the fraction of experts who categorized the image as a Bubble (blue), Irregular or Ambiguous Bubble (orange), and Non-Bubble (red).}
\label{fig:expert_mwp_votes}
\end{figure}

\begin{enumerate} 
\item Only 15\% of these objects were unanimously classified as clear bubbles -- this number increases to about 50\% if all irregular and ambiguous objects are optimistically treated as bubbles. 
\item Depending on how many irregular/ambiguous objects are legitimate bubbles, between 10 and 30\% of objects in the MWP catalog are judged by experts to be interlopers -- that is, objects which are unlikely to be the result of a young star sculpting the surrounding ISM.
\item The most popular category for each object is chosen by 70\% of experts on average. In other words, when using Spitzer images, determining the ``true'' nature of an object in the MWP catalog is subjective at the 30\% level on average.
\end{enumerate}

Is it possible to determine which of the remaining objects in the MWP catalog would likely be rejected as interlopers by experts? The hit rate of each object in the catalog might correlate with expert opinion, and be a useful filter. Likewise, the confidence score computed by Brut might identify interlopers. Figure \ref{fig:expert_mwp_score} plots these these two quantities compared to the expert category. The Y axis of each point denotes the level of consensus for each object -- the percentage of experts who chose the plurality category. Both metrics partially separate Bubbles from the other categories. The Hit Rate is more effective at penalizing orange and red objects, which are confined to Hit Rate $<0.2$. On the other hand, the hit rate is ineffective at isolating bubbles with high expert consensus. The Brut score, is more effective at identifying these high-consensus bubbles, which is apparent by the cluster of blue points in the upper right corner of the plot. We also plot in Figure \ref{fig:expert_mwp_score} a joint score -- the sum of the normalized and mean-subtracted Brut score and hit rate.  This combines the complementary strengths of the individual metrics, and achieves the best separation.

\begin{figure}
\includegraphics{expert_mwp_score}
\caption{The ability for the MWP hit rate and Brut score to predict expert expert classifications for objects in the MWP catalog. Each circle represents an object in the MWP catalog. Circle color depicts the most popular classification, and the y-position indicates the fraction of experts who chose this category. The
black lines show the probability that an object with a given score on the x axis would be classified as a bubble -- this line is obtained via  logistic fit to the data. The Joint metric, which combines the Hit Rate and Brut score, is best able to predict expert classifications.}
\label{fig:expert_mwp_score}
\end{figure}

The expert reclassifications of this sample of Milky Way Project objects can be used to convert an raw score like the joint score to a calibrated probability that an expert would classify an object as a Bubble, given that score. To achieve this, we perform a logistic regression against each of the three scores (hit rate, Brut score, and joint score). The logistic regression only considers whether the plurality category for each object is a Bubble -- the consensus information is not used. The black curves in Figure \ref{fig:expert_mwp_score} trace this curve, and show the predicted probability that an object is a Bubble given the score on the X axis. The last panel in Figure \ref{fig:expert_mwp_score} shows the ideal situation, where a scoring metric perfectly separates two classes of unanimously-categorized objects. The joint score combines the information from citizen scientists and Brut, and comes closest to this ideal. While the logistic curves for the hit rate and joint score look similar, the latter curve is a better fit to the data as judged by the likelihood.

Finally, in Figure \ref{fig:expert_roc} we present the ROC curve of each of the three scores, demonstrating the tradeoff of between false and true positive rate.

\begin{figure}
\includegraphics{expert_roc}
\caption{The receiver operating characteristic curve for the MWP objects reclassified by experts. Each line shows the false positive rate and true positive rate obtained by thresholding the MWP catalog according to the hit rate, Brut score, or joint score. }
\label{fig:expert_roc}
\end{figure}


\subsubsection{Uniform Classification}
The remaining 47 regions in the expert classification were drawn randomly from the full set of fields analyzed during Brut's full-scan of the Spitzer images. A fully random sample from this collection is uninteresting, since the majority of fields are blank areas of the sky. Instead, these 47 images are approximately uniformly distributed in the Brut confidence score. We refer to these images as the ``uniform'' sample. 

The expert vote distribution for these images is shown in Figure \ref{fig:expert_uniform_votes}. There are more non-bubble fields in this sample, but the overall properties are otherwise similar. The average field is classified with a 72\% consensus.

Figure \ref{fig:expert_uniform_score} shows the Brut score for each field in the uniform sample, compared to the plurality category (color) and level of consensus (Y axis). The Brut score does a good job of separating the high consensus objects. Overall, however, the classes overlap more for the uniform sample than for the MWP sample. This is also expected, since all objects in the MWP sample have been previously identified by citizen scientists as potential bubble sites. The uniform sample does not benefit from this information, and is representative of the harder, ``blind'' classification task. As before, a logistic fit can be used to estimate, for objects identified in a blind search, the probability of being a bubble given the Brut score.

\begin{figure}
\includegraphics{expert_uniform_votes}
\caption{The same as Figure \ref{fig:expert_mwp_votes}, for the Uniform sample.}
\label{fig:expert_uniform_votes}
\end{figure}

\begin{figure}
\includegraphics{expert_uniform_score}
\caption{The same as Figure \ref{fig:expert_uniform_score}, for the Uniform sample.}
\label{fig:expert_uniform_score}
\end{figure}

\section{A Probabilistic Bubble Catalog}
Section \ref{sec:expert_mwp} outlines a strategy to combine hit rates with Brut's own confidence score to predict whether
an expert would classify each object in the original MWP catalog as a bubble or not. We have assigned a bubble probability to each
object in MWP catalog based on this analysis. The high-probability subsample of this catalog displays several interesting differences compared to the unfiltered catalog. 

To address this, we have split the catalog into three roughly equally-sized groups: those where $P_{\rm bubble} < 0.5$, $0.5 < P_{\rm bubble} < 0.9$,
and $P_{\rm bubble} > 0.9$. Figure \ref{fig:lon} shows the longitude distribution for each of these groups. The distributions are mostly similar, except at the three longitudes marked with vertical lines. These longitudes exhibit a excess of low-probability bubbles relative to the other groups.  All three fields are in fact sites of large, degree-scale giant HII regions, shown in Figure \ref{fig:wide_fields}. 

The excess of low-probability objects towards $\ell=-43^\circ$ coincides with a large bubble complex, containing bubbles S108-S111 in \cite{Churchwell06}. The region at $\ell=-55^\circ$ coincides with the Dragonfish nebula, studied by \cite{Rahman11}. Finally, the $\ell=-62^\circ$ region contains the G305 star forming complex studied by \cite{Hindson12}.

To determine whether the overabundance of low-probability sources near giant HII regions indicates a failure of the probabilistic scoring or a bias in citizen scientist annotations, the lead author manually inspected the objects in the $\ell=-61^\circ$ bin. The sum of bubble probabilities for the 25 MWP objects in this bin is 8.09, and implies that an expert would identify $\sim 8$ of these objects as real bubbles if the probabilistic score is correct. Roughly 5-6 of these objects are compelling bubble candidates, which suggests that the excess of low-probability bubbles is not due to an overly conservative scoring strategy. Instead this points to a real citizen scientist bias towards over-identification of bubbles in active regions like this. The central source in each of these regions creates a large, complex cavity in the ISM, and fluoresces much of the surrounding material as well. This leads to an abundance of coincidental, arc-like features misidentified as bubbles. The scores provided by Brut are able to identify this problem.

While citizen scientists are prone to over-identify bubbles towards giant HII regions, they are unlikely to identify these large regions themselves. This might be because such objects are to irregular, or simply too big to see well in any single image served during the Milky Way Project. The widest zoom levels at which Brut scans through image data is wider than the widest images seen by citizen scientists, and it recovers some of these regions. For example, Figure \ref{fig:wide_fields} shows in magenta the new regions which Brut scores $>0.2$ on a blind search, that have no match in the MWP catalog. Many of these new identifications are bigger than the biggest previously-cataloged objects.

In discussing the longitude distribution of bubbles, \cite{Simpson12} noted a dip in bubble counts at $|\ell| < 10^\circ$. They postulated that this might be because confusion in this busier region makes identification more difficult. However, this dip is most apparent for low-probability objects. This suggests that the complex background towards the Galactic center (particularly at 24 $\mu$m) might make it more difficult to find coincidental rings.

\begin{figure}
\includegraphics{dist_lon}
\caption{The longitude distribution for the MWP catalog, partitioned according to the bubble probability score.
Vertical black lines identify three degree-scale emission complexes with an overabundance of low-probability objects.}
\label{fig:lon}
\end{figure}


\begin{figure}
\includegraphics{l305}
\includegraphics{l317}
\caption{Two fields with overabundances of low-probability bubbles in the MWP catalog.}
\end{figure}

Figure \ref{fig:lat} shows the latitude distribution for each subsample. The lowest-probability objects display a slightly broader distribution, particularly evident at $|b| > 0.5^\circ$. We suspect that this represents a similar citizen scientist bias, acting in the other direction: because the ambient intensity field falls off quickly away from the mid-plane, random arc-like ISM structures are more likely to catch the eye in these regions. 

\begin{figure}
\includegraphics{dist_lat}
\caption{The latitude distribution for the MWP catalog, partitioned according to the bubble probability score.
Low-probability objects have a slightly wider distribution.}
\label{fig:lat}
\end{figure}

Of the 3661 objects with bubble probability scores, 126 coincide spatially with HII regions from \cite{Anderson11}. As Figure \ref{fig:hii_score} shows, objects associated with these HII regions are strongly skewed towards higher bubble probabilities. This correlation strengthens the argument that the probability score successfully identifies dynamical objects.

\begin{figure}
\includegraphics{dist_hii_score}
\caption{The bubble probability distribution for sources with and without HII region counterparts in the \cite{Anderson11} catalog.}
\label{fig:hii_score}
\end{figure}

\subsection{Evidence for Triggering}

Bubbles are frequently studied in the context of triggered star formation. The material excavated by bubble-blowing stars might induce subsequent generations of star formation. Two main mechanisms for triggered star formation have been proposed: in the collect and collapse model \todo{citation}, material gathered along bubble rims eventually passes a critical density and undergoes gravitational fragmentation and collapse. In the radiatively-driven implosion model, the bubble shock front collides with pre-existing but stable cores, and the resulting compression triggers collapse.

Establishing that any particular region has triggered star formation is difficult. The typical approach is to identify an overdensity of young stars within/on a bubble rim, and/or to look for an inverse age gradient with bubble radius. Such an analysis is often confounded by small numbers of YSOs and ambiguous line-of-sight distances and stellar ages. Furthermore, it is often unclear whether spatial correlations between bubbles and YSOs imply a causal relationship between the two.

Such problems can partially be addressed by correlating YSOs with large bubble catalogs like the MWP catalog -- this does not disambiguate correlation from causation, but it can overcome problems related to small-number statistics. \cite{Thompson12} first applied such an analysis using the C06 catalog, and \cite{Kendrew12} later repeated this on the MWP catalog. This analysis computes an angular correlation function \citep{Landy93, Bradshaw11}, defined by

\begin{equation}
w(\theta) = \frac{N_{BY} - N_{BR_Y} - N_{R_BY} + N_{R_BR_Y}}{N_{R_BR_Y}}
\end{equation}
where $N_{\alpha \beta}$ represents the number of pairs of objects from catalogs $\alpha$ and $\beta$ with a separation of $\theta$, $B$ is a bubble catalog, $Y$ is a YSO catalog, and $R_B$ and $R_Y$ are randomly-distributed bubble and YSO catalogs. Informally, $w(\theta)$ represents the likelihood of finding a YSO at a particular distance from a bubble. 

We reproduce the Figure \todo{???} of \cite{Kendrew12} in Figure\citep{fig:trigger}a. This shows the angular correlation function between the Milky Way Project Large catalog and RMS catalog of YSOs and HII regions \citep{RMS}, as a function of normalized bubble radius. The main signal is a decaying correlation, indicative of the fact that star formation occurs in clustered environments. The prediction from triggered star formation is that there should be an additional peak at $\theta \sim 1$ bubble radius. Such a signal is not obvious in this figure, though \citep{Kendrew} report evidence for a peak for the largest bubbles in the MWP catalog. Likewise, \cite{Thompson12} report a clearer peak when considering the expert-selected bubbles in C06. 

In this paper, we have shown that roughly 30\% of objects in the MWP catalog are interlopers -- random ISM structures incorrectly tagged as bubbles or HII regions. Furthermore, the relative false positive rate increases towards giant HII regions -- precisely the regions where one might expect triggered star formation to be most active. Potentially, interlopers might significantly dilute bubble/YSO correlations in the MWP catalog. Fortunately, our bubble probability scores allow us to identify many of these interlopers, yielding a higher reliability catalog that still has 3x as many bubbles as used by \cite{Thompson12}.

Figure \ref{fig:trigger}b shows the same correlation function, for the subsamples partitioned by bubble probability. This reveals an increasing peak from $0.5 < \theta < 1$, as bubble probability increases. As a population, high-probability bubbles in the MWP catalog are more strongly associated with RMS sources near bubble rims. This is similar to the trend with bubble size that \cite{Kendrew12} reported, but the signal here is both stronger and due to a different subsample of bubbles -- the size distribution of bubbles does not vary significantly between the three probability bins.

\section{Next Steps}
\label{sec:next_steps}
The success of Brut demonstrates the potential synergies that exist between machine learning, professional scientists, and citizen scientists. Note the complementary strengths and weaknesses of each resource:

\begin{enumerate}
\item Professional scientists are best-suited to perform nuanced classification tasks that require domain-specific knowledge. They are also the most resource-limited. 
\item Citizen scientists outnumber professional scientists by orders of magnitude (in the case of Bubble detection, the factor is roughly 1000:1). They are equally capable with the generic aspects of pattern recognition, but do not possess the domain expertise of professionals.
\item Supervised machine learning algorithms have no a-priori pattern recognition ability, and require external training. However, once supplied with this information, computer-driven analyses are reproducible and extremely scalable.
\end{enumerate}

Brut leverages the information provided by astronomers and citizen scientists alike. Citizen scientist input was used for training, and a smaller dataset of high-quality expert information set was used to convert Brut's classifications into calibrated bubble probabilities. The result is a classifier that is both more precise than the raw citizen scientist catalog, and more complete than the best expert-assembled catalog.

Searching Spitzer images for bubbles is a small enough task that citizen scientists were able to perform an exhaustive search. Consequently, the MWP catalog appears to contain most of the bubbles that one can hope to identify from these images. Brut's main benefit is in providing an independent, probabilistic assessment of the MWP catalog, identifing interlopers in the catalog, and adding a small number of bubbles missed by citizen scientists -- particularly bubbles at the largest scales.

However, one can hypothetically envision tools like Brut assisting professional and citizen scientists in real time. For searches for rarer objects in larger datasets, exhaustive human search is infeasible -- both due to boredom and prohibitive data sizes. Had Brut been trained at the start of the Milky Way Project, it would quickly have been able to eliminate many regions as devoid of bubbles. Citizen scientists could have spent more time classifying more ambiguous regions, which is where their input is most valuable (and where the tasks is likely most interesting). These ideas are explored in more depth by \citep{MSR}, and will become increasingly important as data continues to grow.

\section{Conclusion}
\label{sec:conclusion}
We have developed an automatic bubble detector, Brut, using the Random Forest classification algorithm in tandem with a catalog of citizen scientist-identified bubbles in our galaxy. This algorithm is effective at detecting bubbles in Spitzer images. By comparing the confidence scores that Brut computes with expert classifications of a small set of images, we are able to estimate the probability that any given region in Spizter GLIMPSE and MIPSGAL data contains a bubble. We have used Brut to re-assess the objects in the Milky Way Project catalog, and also to perform a full search over GLIMPSE and MIPSGAL images for new bubbles.  Several insights have emerged from this analysis:
\begin{enumerate}
\item Roughly 30\% of the objects in the Milky Way Project catalog are interlopers -- structures which a majority of experts would not consider to be likely HII regions or wind blown bubbles.
\item Brut is able to identify which objects are probable interlopers, and likewise to identify highly probable bubbles. Compared to the MWP catalog as a whole, high-probability bubbles have a narrower latitude distribution \todo{repeat with actual height, for distances}, and are more likely to be associated with HII regions identified by \cite{Anderson11}.
\item The MWP catalog has a higher concentration of low-probability bubbles near giant HII regions, which fluoresce the surrounding ISM and reveal more coincidental circular structures. Citizen scientists are prone to identify these regions as bubbles, whereas experts and Brut do not.
\item Likewise, degree-scale giant HII regions are missing from the MWP catalog, despite their obvious dynamical relevance. This is likely because the images served to citizen scientists did not have a wide enough field of view. Brut detects most of these sources.
\item High probability bubbles  exhibit stronger excesses of YSOs and compact HII regions along bubble rims -- a prediction of triggered star formation theories.
\end{enumerate}

\end{document}
